{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection, Underfitting and Overfitting\n",
    "\n",
    "-------\n",
    "\n",
    "The goal of machine learning is find a learning algorithm (algorithm that is able to learn from data) or simply model, train a model by adjusting the model parameters to get the best possible performance, both on the training (with minimum training error) and the test dat or new inputs (the trained model must be able to generalized well with minimum generalization error or test error) but the challenge in machine learning is how well does the trained model perform not just on the training data, but also on new unseen inputs (test inputs).This is a fundamental problem in machine learning between <b> optimization (the process of adjusting a model parameters to give the best possible performance on the training data) and generalization (how well does the trained model performs on newly unseen data) because a trained model can perform well well on the training dataset but performs poorly on newly unseen data points.</b>\n",
    "\n",
    "\n",
    "# NOTE\n",
    "------\n",
    "<h3 style='color:blue'>The training error is the error of our model as calculated on the training dataset</h3>\n",
    "<h3 style='color:blue'>The generalization error is the expected value of the error on a test or new data points drawn from the same underlying data distribution as our original sample</h3>\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "```\n",
    "The factors determining how well a machine learning algorithm will perform are its ability to:\n",
    "1. Make the training error small.\n",
    "2. Make the gap between training and test error small.\n",
    "These two factors correspond to the two central challenges in machine learning: underfitting and overfitting.\n",
    "\n",
    "(source: From the book, Deep Learning by Ian Goodfellow,Yoshua Bengio and Aaron Courville, page 111) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overftting\n",
    "\n",
    "-------\n",
    "When the complexity of the model is too high (highly flexible models) as compared to the underlying distribution of the data the model is trying to learn from, it tends to learn the noise present in data and is called overfitting. An Overfitted models has it training error much lower than validation error. <b>An overfitted model fails to Generalize well and has high Variance and Low Bias and the techniques used to combat overfitting are called regularization</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfitting\n",
    "----\n",
    "Underfitting occurs when the model can neither obtain sufficiently low error value on the training set nor generalize to new data and has low Variance and high Bias. Underfitted models are not able to reduce the training error. W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "----\n",
    "\n",
    "## Regularization are techniques used to combat overfitting  and this reduces the test error or generalization erro\n",
    "\n",
    "```\n",
    " Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error \n",
    " but not its training error\n",
    " \n",
    " (source: From the book, Deep Learning by Ian Goodfellow,Yoshua Bengio and Aaron Courville, page 120)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  WEIGHT  REGULARIZATION\n",
    "<img src='images/we.jpg'>\n",
    "(source: From the book, Deep Learning by Ian Goodfellow,Yoshua Bengio and Aaron Courville, page 120)\n",
    "<img src='images/weight.jpg'>\n",
    "(source: From the book, Deep Learning with python by Fran√ßois Chollet, page 107)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Weight decay is also known as L2 regularization or ridge regression or Tikhonov regularization\n",
    "\n",
    "<b>L2 regularization is also called weight decay in the context of neural networks</b> prevent the weights from growing too large unless it is really necessary. It can be realized by\n",
    "adding a term to the cost OR objective function that penalizes large weights and is defined as\n",
    "\n",
    "$$\\tilde{\\ell}(w)=\\ell(w) + \\frac{\\lambda}{2}w^{2} $$\n",
    "\n",
    "where $ \\tilde{\\ell}$ is the regularized cost fucbtion $\\ell_{0}$ is an error measure (usually the sum of squared errors) and $\\lambda$ is a hyperparameter chosen ahead of time that controls how weights are penalized. (weights the relative contribution of the norm penalty term $w^{2} $  relative to the standard objective function $\\ell$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "with the corresponding parameter gradient\n",
    "$$\\bigtriangledown \\tilde{\\ell}_{w}(w)=\\bigtriangledown \\ell_{w}(w) + \\lambda w $$\n",
    "\n",
    "The new updated weight after an iteration can be expressed as\n",
    "$$w=w-\\eta \\bigtriangledown\\tilde{\\ell}_{w}(w)=w-\\eta(\\lambda w +\\bigtriangledown \\ell_{w}(w)) $$\n",
    "\n",
    "$$ w=(1- \\eta\\lambda) w -\\eta \\bigtriangledown \\ell_{w}(w)) $$\n",
    "where $\\eta$ is the learning rate\n",
    "\n",
    "\n",
    "The addition of the weight decay term has modified the learning rule of the weight vector by a constant factor on each step just before updating the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear regression, the objective function, sum of squared errors is defined as\n",
    "$$e=(Xw-y)^{T}(Xw-y)$$\n",
    "\n",
    "When L2 regularization is added, the objective function changes to\n",
    "$$e=(Xw-y)^{T}(Xw-y)+ \\frac{\\lambda}{2}w^{2}$$\n",
    "\n",
    "and this the solution $w$ from\n",
    "$$ w=(XX^{T})^{-1}X^{T}y $$\n",
    "\n",
    "$$ To$$\n",
    "\n",
    "$$ w=(XX^{T}   + \\lambda  I )^{-1}X^{T}y $$\n",
    "\n",
    "Where the diagonal entries of this matrix $ \\lambda  I $ correspond to the variance of each input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/lp.jpg'>\n",
    " (source: From the book am using: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 155-156)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on the effects of weight regularization \n",
    "<a href='papers/563-a-simple-weight-decay-can-improve-generalization.pdf'>A Simple Weight Decay Can Improve Generalization by Anders Krogh and John A. Hertz</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T22:57:55.990446Z",
     "start_time": "2020-09-07T22:57:55.947447Z"
    }
   },
   "source": [
    "# High-Dimensional Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T22:35:48.369149Z",
     "start_time": "2020-09-09T22:35:45.093554Z"
    }
   },
   "outputs": [],
   "source": [
    "import d2l\n",
    "from mxnet import gluon, npx,np,init,autograd\n",
    "from mxnet.gluon import nn\n",
    "import mxnet\n",
    "%matplotlib inline\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/highp.jpg'>\n",
    "(source: From the book am using: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T22:35:48.397458Z",
     "start_time": "2020-09-09T22:35:48.369149Z"
    }
   },
   "outputs": [],
   "source": [
    "n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5\n",
    "true_w, true_b=np.zeros((num_inputs,1))*0.01,0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T22:35:48.418458Z",
     "start_time": "2020-09-09T22:35:48.402454Z"
    }
   },
   "outputs": [],
   "source": [
    "features, y_train= d2l.synthetic_data(true_w, true_b, n_train)\n",
    "x_test, y_test= d2l.synthetic_data(true_w, true_b, n_train)\n",
    "train_iter = d2l.load_array((features, y_train), batch_size)\n",
    "test_data = d2l.synthetic_data(true_w, true_b, n_test)\n",
    "test_iter = d2l.load_array(test_data, batch_size, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# defining our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T22:35:50.520191Z",
     "start_time": "2020-09-09T22:35:50.513188Z"
    }
   },
   "outputs": [],
   "source": [
    "net=gluon.nn.Sequential()\n",
    "net.add(gluon.nn.Dense(20,activation='relu'))\n",
    "net.add(gluon.nn.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T22:35:57.505727Z",
     "start_time": "2020-09-09T22:35:57.494703Z"
    }
   },
   "outputs": [],
   "source": [
    "net.initialize(mxnet.init.Normal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# l2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T22:36:01.163284Z",
     "start_time": "2020-09-09T22:36:01.159281Z"
    }
   },
   "outputs": [],
   "source": [
    "l2_loss=gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T22:36:02.292546Z",
     "start_time": "2020-09-09T22:36:02.212378Z"
    }
   },
   "outputs": [],
   "source": [
    "# The weight parameter has been decayed. Weight names generally end with \"weight\".\n",
    "trainer_w = gluon.Trainer(net.collect_params('.*weight'), 'sgd',{'learning_rate': 0.01, 'wd': 2})\n",
    "    # The bias parameter has not decayed. Bias names generally end with \"bias\".\n",
    "    # The biases require less data to fit accurately than the weights\n",
    "trainer_b = gluon.Trainer(net.collect_params('.*bias'), 'sgd',{'learning_rate': 0.01})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T22:36:04.861703Z",
     "start_time": "2020-09-09T22:36:03.672466Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, training loss: 0.006028, validation loss: 0.025253\n",
      "epoch 1, training loss: 0.005488, validation loss: 0.022826\n",
      "epoch 2, training loss: 0.005018, validation loss: 0.020692\n",
      "epoch 3, training loss: 0.004603, validation loss: 0.018806\n",
      "epoch 4, training loss: 0.004223, validation loss: 0.017128\n",
      "epoch 5, training loss: 0.003882, validation loss: 0.015629\n",
      "epoch 6, training loss: 0.003573, validation loss: 0.014283\n",
      "epoch 7, training loss: 0.003304, validation loss: 0.013072\n",
      "epoch 8, training loss: 0.003053, validation loss: 0.011978\n",
      "epoch 9, training loss: 0.002824, validation loss: 0.010989\n",
      "epoch 10, training loss: 0.002617, validation loss: 0.010092\n",
      "epoch 11, training loss: 0.002427, validation loss: 0.009277\n",
      "epoch 12, training loss: 0.002255, validation loss: 0.008534\n",
      "epoch 13, training loss: 0.002096, validation loss: 0.007858\n",
      "epoch 14, training loss: 0.001949, validation loss: 0.007242\n",
      "epoch 15, training loss: 0.001814, validation loss: 0.006680\n",
      "epoch 16, training loss: 0.001691, validation loss: 0.006166\n",
      "epoch 17, training loss: 0.001577, validation loss: 0.005697\n",
      "epoch 18, training loss: 0.001472, validation loss: 0.005269\n",
      "epoch 19, training loss: 0.001376, validation loss: 0.004877\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20# Number of iterations\n",
    "batch_size=5\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss=0\n",
    "    for X,y in train_iter:\n",
    "        with autograd.record():\n",
    "            y_hat=net(X)\n",
    "            loss_val=l2_loss(y_hat,y)# Minibatch loss in X and y\n",
    "        loss_val.backward() # Compute gradient on l with respect to [w, b]\n",
    "        trainer_w.step(batch_size=batch_size)\n",
    "        trainer_b.step(batch_size=batch_size)\n",
    "        train_loss +=loss_val.mean().asnumpy()\n",
    "    val_loss=0  \n",
    "    for x_test,y_test in test_iter:\n",
    "        pred=net(x_test)\n",
    "        va_loss=l2_loss(pred,y_test)\n",
    "        val_loss+=va_loss.mean().asnumpy()\n",
    "    print('epoch %d, training loss: %f, validation loss: %f' % (epoch, train_loss, val_loss))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
