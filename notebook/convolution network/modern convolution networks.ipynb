{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#LENET-NETWORK\" data-toc-modified-id=\"LENET-NETWORK-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>LENET NETWORK</a></span></li><li><span><a href=\"#ALEXNET\" data-toc-modified-id=\"ALEXNET-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>ALEXNET</a></span></li><li><span><a href=\"#VGG-Blocks\" data-toc-modified-id=\"VGG-Blocks-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>VGG Blocks</a></span></li><li><span><a href=\"#Network-in-Network-(NiN)-BLOCK\" data-toc-modified-id=\"Network-in-Network-(NiN)-BLOCK-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Network in Network (NiN) BLOCK</a></span></li><li><span><a href=\"#INCEPTION\" data-toc-modified-id=\"INCEPTION-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>INCEPTION</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet\n",
    "from mxnet import gluon,npx,np,autograd\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LENET NETWORK\n",
    "<img src='images/lenet.jpg'>\n",
    "<img src='../images/lenet.svg'>\n",
    "\n",
    " (source: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 252-3)\n",
    "  \n",
    "\n",
    "To read more on LENET visit :\n",
    "\n",
    "<a href='http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf'>GradientBased Learning Applied to Document Recognition</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet=nn.Sequential()\n",
    "lenet.add(nn.Conv2D(channels=6,padding=2,kernel_size=5,activation='sigmoid'),\n",
    "          nn.AvgPool2D(pool_size=2,strides=2),\n",
    "          nn.Conv2D(channels=16,kernel_size=5,activation='sigmoid'),\n",
    "          nn.AvgPool2D(pool_size=2,strides=2),\n",
    "         nn.Dense(120, activation='sigmoid'),\n",
    "         nn.Dense(84, activation='sigmoid'),\n",
    "         nn.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As compared to the original network, we took the liberty of replacing the Gaussian activation in\n",
    "the last layer by a regular dense layer, which tends to be significantly more convenient to train.\n",
    "Other than that, this network matches the historical definition of LeNet5.\n",
    "Next, let us take a look of an example. As shown in Fig. 6.6.2, we feed a single-channel example\n",
    "of size 28 Ã— 28 into the network and perform a forward computation layer by layer printing the\n",
    "output shape at each layer to make sure we understand what is happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv0 output shape:\t (1, 6, 28, 28)\n",
      "pool0 output shape:\t (1, 6, 14, 14)\n",
      "conv1 output shape:\t (1, 16, 10, 10)\n",
      "pool1 output shape:\t (1, 16, 5, 5)\n",
      "dense0 output shape:\t (1, 120)\n",
      "dense1 output shape:\t (1, 84)\n",
      "dense2 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.uniform(size=(1, 1, 28, 28))\n",
    "lenet.initialize()\n",
    "for layer in lenet:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALEXNET\n",
    "<img src='../images/alexnet.jpg'>\n",
    " (source: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 261)\n",
    "  \n",
    "To read more on Alexnet visit :\n",
    "\n",
    "<a href='https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf'>ImageNet Classification with Deep Convolutional\n",
    "Neural Networks\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet=nn.Sequential()\n",
    "alexnet.add(nn.Conv2D(channels=96,kernel_size=11,strides=4,activation='relu'),\n",
    "            nn.MaxPool2D(pool_size=3,strides=2),\n",
    "            nn.Conv2D(channels=256,kernel_size=5,padding=2,activation='relu'),\n",
    "            nn.MaxPool2D(pool_size=3,strides=2),\n",
    "            nn.Conv2D(channels=384,kernel_size=3,padding=1,activation='relu'),\n",
    "            nn.Conv2D(channels=384,kernel_size=3,padding=1,activation='relu'),\n",
    "            nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n",
    "            nn.MaxPool2D(pool_size=3, strides=2),\n",
    "            # Here, the number of outputs of the fully connected layer is several\n",
    "            # times larger than that in LeNet. Use the dropout layer to mitigate\n",
    "            # overfitting\n",
    "            nn.Dense(4096, activation=\"relu\"), nn.Dropout(0.5),\n",
    "            nn.Dense(4096, activation=\"relu\"), nn.Dropout(0.5),\n",
    "            # Output layer. Since we are using Fashion-MNIST, the number of\n",
    "            # classes is 10, instead of 1000 as in the paper\n",
    "            nn.Dense(10)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a single-channel data instance with both height and width of 224 to observe the output shape of each layer. It matches our diagram above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2 output shape:\t (1, 96, 54, 54)\n",
      "pool2 output shape:\t (1, 96, 26, 26)\n",
      "conv3 output shape:\t (1, 256, 26, 26)\n",
      "pool3 output shape:\t (1, 256, 12, 12)\n",
      "conv4 output shape:\t (1, 384, 12, 12)\n",
      "conv5 output shape:\t (1, 384, 12, 12)\n",
      "conv6 output shape:\t (1, 256, 12, 12)\n",
      "pool4 output shape:\t (1, 256, 5, 5)\n",
      "dense3 output shape:\t (1, 4096)\n",
      "dropout0 output shape:\t (1, 4096)\n",
      "dense4 output shape:\t (1, 4096)\n",
      "dropout1 output shape:\t (1, 4096)\n",
      "dense5 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.uniform(size=(1, 1, 224, 224))\n",
    "alexnet.initialize()\n",
    "for layer in alexnet:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG Blocks\n",
    "<img src='../images/vvg.jpg'>\n",
    "The function takes two arguments corresponding to the number of convolutional layers num_convs and the number of output channels num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, num_channels):\n",
    "    blk = nn.Sequential()\n",
    "    for _ in range(num_convs):\n",
    "        blk.add(nn.Conv2D(num_channels, kernel_size=3,padding=1, activation='relu'))\n",
    "    blk.add(nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original VGG network had 5 convolutional blocks, among which the first two have one convolutional layer each and the latter three contain two convolutional layers each. The first block has\n",
    "64 output channels and each subsequent block doubles the number of output channels, until that\n",
    "number reaches 512. Since this network uses 8 convolutional layers and 3 fully-connected layers,\n",
    "it is often called VGG-11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements VGG-11. This is a simple matter of executing a for loop over\n",
    "conv_arch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    net=nn.Sequential()\n",
    "    for (num_convs, num_channels) in conv_arch:\n",
    "        net.add(vgg_block(num_convs,num_channels))\n",
    "    net.add(nn.Dense(4096, activation='relu'), nn.Dropout(0.5),nn.Dense(4096, activation='relu'),\n",
    "            nn.Dropout(0.5),nn.Dense(10))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will construct a single-channel data example with a height and width of 224 to observe\n",
    "the output shape of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11 = vgg(conv_arch)\n",
    "vgg11.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential3 output shape:\t (1, 64, 112, 112)\n",
      "sequential4 output shape:\t (1, 128, 56, 56)\n",
      "sequential5 output shape:\t (1, 256, 28, 28)\n",
      "sequential6 output shape:\t (1, 512, 14, 14)\n",
      "sequential7 output shape:\t (1, 512, 7, 7)\n",
      "dense6 output shape:\t (1, 4096)\n",
      "dropout2 output shape:\t (1, 4096)\n",
      "dense7 output shape:\t (1, 4096)\n",
      "dropout3 output shape:\t (1, 4096)\n",
      "dense8 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = np.random.uniform(size=(1, 1, 224, 224))\n",
    "for blk in vgg11:\n",
    "    X = blk(X)\n",
    "    print(blk.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Network in Network (NiN) BLOCK\n",
    "<img src='../images/nin.jpg'/>\n",
    "The NiN block consists of one convolutional layer followed by two 1 Ã— 1 convolutional layers that\n",
    "act as per-pixel fully-connected layers with ReLU activations. The convolution width of the first\n",
    "layer is typically set by the user. The subsequent widths are fixed to 1 Ã— 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nin_block(num_channels, kernel_size, strides, padding):\n",
    "    blk = nn.Sequential()\n",
    "    blk.add(nn.Conv2D(num_channels, kernel_size, strides, padding,activation='relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1, activation='relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1, activation='relu'))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nin = nn.Sequential()\n",
    "nin.add(nin_block(96, kernel_size=11, strides=4, padding=0),nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        nin_block(256, kernel_size=5, strides=1, padding=2),nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        nin_block(384, kernel_size=3, strides=1, padding=1),nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        nn.Dropout(0.5),\n",
    "        # There are 10 label classes\n",
    "        nin_block(10, kernel_size=3, strides=1, padding=1),\n",
    "         # The global average pooling layer automatically sets the window shape\n",
    "          # to the height and width of the input\n",
    "           nn.GlobalAvgPool2D(),\n",
    "           # Transform the four-dimensional output into two-dimensional output\n",
    "           # with a shape of (batch size, 10)\n",
    "        nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential9 output shape:\t (1, 96, 54, 54)\n",
      "pool10 output shape:\t (1, 96, 26, 26)\n",
      "sequential10 output shape:\t (1, 256, 26, 26)\n",
      "pool11 output shape:\t (1, 256, 12, 12)\n",
      "sequential11 output shape:\t (1, 384, 12, 12)\n",
      "pool12 output shape:\t (1, 384, 5, 5)\n",
      "dropout4 output shape:\t (1, 384, 5, 5)\n",
      "sequential12 output shape:\t (1, 10, 5, 5)\n",
      "pool13 output shape:\t (1, 10, 1, 1)\n",
      "flatten0 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.uniform(size=(1, 1, 224, 224))\n",
    "nin.initialize()\n",
    "for layer in nin:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INCEPTION\n",
    "<img src=\"../images/inception.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_block(nn.Block):\n",
    "    def __init__(self,c1,c2,c3,c4,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Path 1 is a single 1 x 1 convolutional layer\n",
    "        self.p1_1=nn.Conv2D(c1,kernel_size=1,activation='relu')\n",
    "        # Path 2 is a 1 x 1 convolutional layer followed by a 3 x 3\n",
    "        # convolutional layer\n",
    "        self.p2_1=nn.Conv2D(c2[0],kernel_size=1,activation='relu')\n",
    "        self.p2_2=nn.Conv2D(c2[1],kernel_size=3,padding=1,activation='relu')\n",
    "        # Path 2 is a 1 x 1 convolutional layer followed by a 5 x 5\n",
    "        # convolutional layer\n",
    "        self.p3_1=nn.Conv2D(c3[0],kernel_size=1,activation='relu')\n",
    "        self.p3_2=nn.Conv2D(c3[1],kernel_size=5,padding=2,activation='relu')\n",
    "        # Path 4 is a 3 x 3 maximum pooling layer followed by a 1 x 1\n",
    "        # convolutional layer\n",
    "        self.p4_1=nn.MaxPool2D(pool_size=3,padding=1,strides=1)\n",
    "        self.p4_2=nn.Conv2D(c4,kernel_size=1,activation='relu')\n",
    "    def forward(self,x):\n",
    "        p1=self.p1_1(x)\n",
    "        p2=self.p2_2(self.p2_1(x))\n",
    "        p3=self.p3_2(self.p3_1(x))\n",
    "        p4=self.p4_2(self.p4_1(x))\n",
    "        return np.concatenate((p1,p2,p3,p4),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet uses a stack of a total of 9 inception blocks and global average pooling to generate its estimates. Maximum pooling between inception blocks reduced the\n",
    "dimensionality. The first part is identical to AlexNet and LeNet, the stack of blocks is inherited\n",
    "from VGG and the global average pooling avoids a stack of fully-connected layers at the end. The\n",
    "architecture is depicted below\n",
    "<img src=\"../images/inception1.jpg\" />\n",
    "<img src=\"../images/inception2.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception=nn.Sequential()\n",
    "inception.add(nn.Conv2D(64,kernel_size=7,strides=2,padding=1,activation='relu'),\n",
    "              nn.MaxPool2D(pool_size=3,padding=1,strides=2),\n",
    "              \n",
    "              nn.Conv2D(64,kernel_size=1,activation='relu'),\n",
    "              nn.Conv2D(192,kernel_size=3,padding=1,activation='relu'),\n",
    "              nn.MaxPool2D(pool_size=3,padding=1,strides=2),\n",
    "              # inception(3a)\n",
    "              Inception_block(c1=64,c2=(96,128),c3=(16,32),c4=32),\n",
    "              # inception(3b)\n",
    "              Inception_block(c1=128,c2=(128,192),c3=(32,96),c4=64),\n",
    "              nn.MaxPool2D(pool_size=3,strides=2,padding=1),\n",
    "              # inception(4a)\n",
    "              Inception_block(c1=192,c2=(96,208),c3=(16,48),c4=64),\n",
    "              # inception(4b)\n",
    "              Inception_block(c1=160,c2=(112,224),c3=(24,64),c4=64),\n",
    "              # inception(4c)\n",
    "              Inception_block(c1=128,c2=(128,256),c3=(24,64),c4=64),\n",
    "              # inception(4d)\n",
    "              Inception_block(112,(144,288),(32,64),64),\n",
    "              # inception(4e)\n",
    "              Inception_block(256,(160,320),(32,128),128),\n",
    "              nn.MaxPool2D(pool_size=3,strides=2,padding=1),\n",
    "              # inception(5a)\n",
    "              Inception_block(256, (160, 320), (32, 128), 128),\n",
    "              # inception(5b)\n",
    "              Inception_block(384, (192, 384), (48, 128), 128),\n",
    "              nn.GlobalAvgPool2D(),\n",
    "              nn.Dense(10)\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv28 output shape:\t (1, 64, 46, 46)\n",
      "pool14 output shape:\t (1, 64, 23, 23)\n",
      "conv29 output shape:\t (1, 64, 23, 23)\n",
      "conv30 output shape:\t (1, 192, 23, 23)\n",
      "pool15 output shape:\t (1, 192, 12, 12)\n",
      "inception_block0 output shape:\t (1, 256, 12, 12)\n",
      "inception_block1 output shape:\t (1, 480, 12, 12)\n",
      "pool18 output shape:\t (1, 480, 6, 6)\n",
      "inception_block2 output shape:\t (1, 512, 6, 6)\n",
      "inception_block3 output shape:\t (1, 512, 6, 6)\n",
      "inception_block4 output shape:\t (1, 512, 6, 6)\n",
      "inception_block5 output shape:\t (1, 528, 6, 6)\n",
      "inception_block6 output shape:\t (1, 832, 6, 6)\n",
      "pool24 output shape:\t (1, 832, 3, 3)\n",
      "inception_block7 output shape:\t (1, 832, 3, 3)\n",
      "inception_block8 output shape:\t (1, 1024, 3, 3)\n",
      "pool27 output shape:\t (1, 1024, 1, 1)\n",
      "dense9 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.uniform(size=(1, 1, 96, 96))\n",
    "inception.initialize()\n",
    "for layer in inception:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
