{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# information\n",
    "----\n",
    " information represents the degree of surprise or the abstract possibility of an event\n",
    " \n",
    " Accordding to Shannon an informative message is a message with a very low chance of occurrening. in contrast, a predictable message has a small amount of information. One is not surprised to receive it. There is no information, if there is no randomness or uncertainty because if we have an absolute knowledge about event and we are certain that the event will shappen, then that event does not convey any information.\n",
    "\n",
    "Shannon introduced the terminology bit as the unit of information\n",
    "\n",
    "# note\n",
    "information can be measured in bits (base 2), natural units (nats) (base e), or hartleys(base 10)\n",
    "depending on the\n",
    "base of the logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T01:47:36.132718Z",
     "start_time": "2020-04-04T01:47:30.882059Z"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import np,npx\n",
    "from mxnet.metric import NegativeLogLikelihood\n",
    "from mxnet.ndarray import nansum\n",
    "import random\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-information\n",
    "\n",
    "Self-information quantifies the level of information or surprise associated with one particular outcome or event of a random variable,\n",
    "\n",
    "Using binary encoding any information is encoded by a series of 0 and 1. And hence, a series of binary digits of length n contains n bits of information\n",
    "\n",
    "suppose that for any series of codes, each 0 or 1 occurs with a probability of $\\frac{1}{2}$. Hence, an\n",
    "event X with a series of codes of length n, occurs with a probability of $\\frac{1}{2^{n}}$ .This series contains $n$ bits of information.\n",
    "\n",
    "# NOTE The amount of information conveyed by each individual events  are considered as a random variable\n",
    "Let X be a discrete random variable defined by $\\{ x_{1},x_{2}, \\cdots ,x_{n} \\}$ and with probability $ \\{p(X=x_{1}),p(X=x_{2}), \\cdots ,p(X=x_{n})\\}$ .\n",
    "To measure the amount of information provided by an event $X$ Shannon gave the answer by defining self-information as\n",
    "\n",
    "$$I(X=x_{i}) =log_{2} \\frac{1}{p(X=x_{i})} =− log_{2}(p(X=x_{i})$$\n",
    "as the bits of information we have received from an event X\n",
    "\n",
    "WHERE $$p=\\frac{1}{2^{n}}$$\n",
    "\n",
    "## PROPERTIES\n",
    "----\n",
    "$$ I(X) \\geqslant 0 $$\n",
    "$$ I(X)=0  \\ if \\  p(X)=1 $$\n",
    "$$ if \\ p(X)< p(Y) \\ then I(X) > I(Y)  $$\n",
    "$$ p(X) \\rightarrow 0, I(X)\\rightarrow  +\\inf $$\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    " For example, the code “0010” has a self-information\n",
    "$$I(\"0010\")=-log_{2}(p(\"0010\"))=-log_{2}\\frac{1}{2^{4}}=  4bits \\ of \\ information$$\n",
    "\n",
    " and “100110” has a self-information\n",
    "$$I(“100110”)=-log_{2}(p(“100110”))=-log_{2}\\frac{1}{2^{6}}=6bits \\ of \\ information$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T01:47:50.551390Z",
     "start_time": "2020-04-04T01:47:50.534386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log2(1/2**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T01:47:52.772265Z",
     "start_time": "2020-04-04T01:47:52.765285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T01:47:53.253713Z",
     "start_time": "2020-04-04T01:47:53.244711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log2(1/2**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T01:47:53.323711Z",
     "start_time": "2020-04-04T01:47:53.319716Z"
    }
   },
   "outputs": [],
   "source": [
    "def self_information(p):\n",
    "    return -np.log2(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T01:47:55.073100Z",
     "start_time": "2020-04-04T01:47:55.065096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_information(1/64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " whereas the entropy\n",
    "quantifies how \"informative\" or\n",
    "\"surprising\" the entire random variable is,\n",
    "averaged on all its possible outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy (Average Self-information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information entropy, often just entropy, is a basic quantity in information\n",
    "theory associated to any random variable, which measures the average uncertainty (randomness) in the random variable. It is the number of bits on average required to describe the random\n",
    "variable and can be interpreted as the average level of \"information\", \"surprise\", or \"uncertainty inherent in the variable's possible outcomes\n",
    "\n",
    "\n",
    "Self-information deals only with a single outcome whilst $entropy$ quantify the amount of uncertainty in an entire probability distribution\n",
    "\n",
    "# NOTE\n",
    "$$log=log_{2}$$\n",
    "$x \\sim p$ is read random variable x has a distribution p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is defined as\n",
    "\n",
    "$$H(X)=-E_{x\\sim p}[\\log p(x)]  $$\n",
    "if X is discrete\n",
    "$$H(X)=\\sum_{i}p(X=x_{i}) I(X)=-\\sum_{i}p(X=x_{i}) \\log p(X=x_{i}) $$\n",
    "\n",
    "if X is continuous, entropy is refered as differential entropy and is defined as\n",
    "$$H(X)=\\int_{x}p(X=x_{i})I(X)dx=-\\int_{x}p(X=x_{i}) \\log p(X=x_{i})d(x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T01:57:28.288579Z",
     "start_time": "2020-04-04T01:57:28.280581Z"
    }
   },
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    entropy=-p*np.log2(p)\n",
    "    out=nansum(entropy.as_nd_ndarray())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T01:57:28.418578Z",
     "start_time": "2020-04-04T01:57:28.367581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1.6854753]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(np.array([0.1, 0.5, 0.1, 0.3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we have a students race with four student\n",
    "taking part with probabilities $\\{ \\frac{1}{4}, \\frac{1}{2},\\frac{1}{8},\\frac{1}{16},\\frac{1}{16} \\}$\n",
    "are . We can calculate the entropy of the students as\n",
    "\n",
    "$$ -\\frac{1}{4}log\\frac{1}{4} -\\frac{1}{2}log\\frac{1}{2}-\\frac{1}{8}log\\frac{1}{8}-2\\frac{1}{16}log\\frac{1}{16}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T01:57:30.020685Z",
     "start_time": "2020-04-04T01:57:30.011691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.875"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1/4*np.log2(1/4) - 1/2*np.log2(1/2)-1/8*np.log2(1/8)-2*1/16*np.log2(1/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1.875]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([1/4,1/2,1/8,1/16,1/16])\n",
    "entropy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ent(p):\n",
    "    entropy=-p*np.log2(p)\n",
    "    return entropy.sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.875)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Entropy \n",
    "\n",
    "the entropy of a binary source is defined as\n",
    "\n",
    "$$ H(X)=-p(x)log \\ p(x) -(1-p(x)) log \\ (1-p(x)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.75"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Entropy\n",
    "\n",
    "The joint entropy $H(X, Y )$ of a pair of random variables $(X, Y )$ with a joint distribution $P_{X,Y}(x, y)$ is defined as\n",
    "\n",
    "$$H(X,Y)=E_{x \\sim p}[log \\ P_{X,Y}(x,y)]$$\n",
    "\n",
    "for a pair (X, Y ) of discrete random variables\n",
    "\n",
    "$$H(X,Y)=-\\sum_{x}\\sum_{y}P_{X,Y}(x,y)log \\ P_{X,Y}(x,y)$$\n",
    "\n",
    "for pair (X, Y ) of continuous random variables, the differential\n",
    "joint entropy is defined as\n",
    "\n",
    "\n",
    "$$H(X,Y)=-\\int_{x,y}P_{X,Y}(x,y)log \\ P_{X,Y}(x,y) dxdy$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_entropy(p_xy):\n",
    "    joint_ent = -p_xy * np.log2(p_xy)\n",
    "    # nansum will sum up the non-nan number\n",
    "    out = nansum(joint_ent.as_nd_ndarray())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1.6854753]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_entropy(np.array([[0.1, 0.5], [0.1, 0.3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
